{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan-mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6NX08GRfRli",
        "colab_type": "text"
      },
      "source": [
        "#GAN - MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzajUrQPpHKA",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0xXTO7ijGHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdLFetGwpJka",
        "colab_type": "text"
      },
      "source": [
        "##Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT0Pp1UljUPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Initialize Batch Size\n",
        "batch_size = 64\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Load Dataset\n",
        "train_dataset = datasets.MNIST(root='data', train=True, download =True, transform=transform)\n",
        "\n",
        "# Prepare the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_workers )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh0dtKrSpL81",
        "colab_type": "text"
      },
      "source": [
        "##Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouajGzj3lD-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "b219c6e3-d43a-40a4-9bec-f807633c6c20"
      },
      "source": [
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "\n",
        "# get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "fig = plt.figure(figsize = (3,3)) \n",
        "px = fig.add_subplot(111)\n",
        "px.imshow(img, cmap='gray')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f17cc929668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADFCAYAAAARxr1AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JLQbBVsfIee",
        "colab_type": "text"
      },
      "source": [
        "#GAN Model\n",
        "##Discriminator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvZKA89_gFpJ",
        "colab_type": "text"
      },
      "source": [
        "It is a linear classifier with 2 hidden layers. This network uses leaky ReLU as an activation function.\n",
        "A leaky ReLU is a normal ReLU function but it return small non-zero value for negative inputs. We use leaky ReLU so gradients can backward propagate without hinderance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-NsvpxokELz",
        "colab_type": "text"
      },
      "source": [
        "Using a BCEWithLogitsLoss which combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable. Source: [Documentation](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIXXb3PCobrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(Discriminator, self).__init__()\n",
        "    \n",
        "    # Layers\n",
        "    self.conv1 = nn.Linear(input_size, hidden_dim*4)    # Input Layer\n",
        "    self.conv2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
        "    self.conv3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "    self.conv4 = nn.Linear(hidden_dim, output_size)  # Final Layer\n",
        "    \n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # Flatten image\n",
        "    x = x.view(-1, 28*28)\n",
        "      \n",
        "    x = F.leaky_relu(self.conv1(x), 0.2)   # (input, negative_slope)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "     \n",
        "    # Final Layer\n",
        "    out = self.conv4(x)\n",
        "    return out\n",
        "    \n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRe74OSRxRu3",
        "colab_type": "text"
      },
      "source": [
        "##Add Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atwr3f4-xVzb",
        "colab_type": "text"
      },
      "source": [
        "Works like the Discriminator but use tanh as an activation function in the output layer.\n",
        "Generator works best when tanh is scaled between -1 and 1.\n",
        "So we will have to scale our real input images to have pixel values between -1 and 1 when we train the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cf2_ZpmwBK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(Generator, self).__init__()\n",
        "    \n",
        "    # Layers\n",
        "    self.conv1 = nn.Linear(input_size, hidden_dim)    # Input Layer\n",
        "    self.conv2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
        "    self.conv3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n",
        "    self.conv4 = nn.Linear(hidden_dim*4, output_size)  # Final Layer\n",
        "    \n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.leaky_relu(self.conv1(x), 0.2)   # (input, negative_slope)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "     \n",
        "    # Final Layer\n",
        "    output = F.tanh(self.conv4(x))\n",
        "      \n",
        "    return output\n",
        "    \n",
        "   \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhKJivAX1Sz6",
        "colab_type": "text"
      },
      "source": [
        "##Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT5uVBitzZlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator Hyperarameters\n",
        "\n",
        "# Size of input image\n",
        "input_size = 784\n",
        "# Size of discriminator output\n",
        "d_output_size = 1\n",
        "# Size of last hidden layer in discriminator (layer 3)\n",
        "d_hidden_size = 32\n",
        "\n",
        "\n",
        "# Generator Hyperparamters\n",
        "\n",
        "# Size of latent vector to give to Generator (reffered as noise)\n",
        "z_size = 100\n",
        "# Size of discriminator output (generated image)\n",
        "g_output_size = 784\n",
        "# Size of first hidden layer\n",
        "g_hidden_size = 32\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tPGG7ZJ9m6e",
        "colab_type": "text"
      },
      "source": [
        "##Building Network\n",
        "Instantiating the current network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GvnM5aq9Smb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "71865080-1edf-4b98-dfae-10e21438a91e"
      },
      "source": [
        "D = Discriminator(input_size, d_hidden_size, d_output_size)\n",
        "G = Generator(z_size, g_hidden_size, g_output_size)\n",
        "\n",
        "print(D)\n",
        "print()\n",
        "print(G)\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (conv1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (conv2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (conv3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (conv4): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.3)\n",
            ")\n",
            "\n",
            "Generator(\n",
            "  (conv1): Linear(in_features=100, out_features=32, bias=True)\n",
            "  (conv2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (conv3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (conv4): Linear(in_features=128, out_features=784, bias=True)\n",
            "  (dropout): Dropout(p=0.3)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc7Dhd7rxBPl",
        "colab_type": "text"
      },
      "source": [
        "##Discriminator and Generator Losses\n",
        "###Discriminator Loss\n",
        "*  Total loss = real loss + fake loss (d_loss = d_real_loss + d_fake_loss)\n",
        "*   Output = 1 for real images\n",
        "*   Output = 0 for fake images\n",
        "*   Using BCEWithLogitsLoss\n",
        "*   To generalize better we reduce the real label classification limit to 0.9 - 1.0, using *smooth* parmeter for this\n",
        "*   Generator outputs images, we want D(fake_images) = 0\n",
        "\n",
        "###Generator Loss\n",
        "*   Similar to Discriminator only flipped, D(fake_images) = 1\n",
        "*   The labels are flipped to represent that the generator is trying to fool the discriminator into thinking that the images it generates (fakes) are real!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EltQcy1e-TFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculating Loss\n",
        "def real_loss(D_out, smooth=False):\n",
        "  batch_Size = D.out.size(0)\n",
        "  # label smoothing\n",
        "  if smooth:\n",
        "    # if smooth, real_labels = 0.9\n",
        "    labels = torch.ones(batch_size) * 0.9\n",
        "  else:\n",
        "    labels = torch.ones(batch_size)\n",
        "    \n",
        "    \n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # calculating loss\n",
        "    loss = criterion(D_out_squeeze(), labels)\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "  \n",
        "def fake_loss(D_out):\n",
        "  batch_size = D_out.size(0)\n",
        "  labels = torch.zeros(batch_size)  #fake_labels = 0\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  # calculating loss\n",
        "  loss = criterion(D_out.squeeze(), labels)\n",
        "  return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjUBhCqpl3Mc",
        "colab_type": "text"
      },
      "source": [
        "##Optimizers\n",
        "Updating generator and discriminator separately. Assigning two separate Adam optimizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqJbeEOBL55d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Learning Rate\n",
        "lr = 0.002\n",
        "\n",
        "# Optimizers\n",
        "d_optimizer = optim.Adam(D.parameters(), lr)\n",
        "g_optimizer = optim.Adam(G.parameters(), lr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy2h4rrQfudK",
        "colab_type": "text"
      },
      "source": [
        "##Training\n",
        "We will use real_loss and fake_loss to calculate losses in all the following cases\n",
        "###Discriminator\n",
        "*   Compute discriminator loss on real, training images\n",
        "*   Generate fake images\n",
        "*   Compute loss on fake (generated) images\n",
        "*   Add real and fake loss\n",
        "*   Perform backpropagation + an optimization step to update discriminator weights\n",
        "\n",
        "###Generator\n",
        "*   Generate fake images\n",
        "*   Compute discriminator loss on fake (generated) images\n",
        "*   Perform backpropagation + an optimization step to update generator weights\n",
        "\n",
        "###Saving Samples\n",
        "As we train, we printout loss statistics and save images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTX72hbaL4-c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "e51daa8f-69cd-4900-9743-423ae824f11c"
      },
      "source": [
        "import pickle as pkl\n",
        "\n",
        "# Defining Epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# To keep track of loss and generated \"fake\" images\n",
        "samples = []\n",
        "losses = []\n",
        "\n",
        "print_every = 400\n",
        "\n",
        "# Getting fixed data fro sampling (constant unaltered throughtout training, allows to inspect the model's performance)\n",
        "\n",
        "sample_size = 16\n",
        "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
        "fixed_z = torch.from_numpy(fixed_z).float()\n",
        "\n",
        "# train the network\n",
        "D.train()\n",
        "G.train()\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  for batch_i, (real_images, _) in enumerate(train_loader):\n",
        "    \n",
        "    batch_size = real_images.size(0)\n",
        "    \n",
        "    \n",
        "    # Rescale Images from [0,1) to [-1, 1]\n",
        "    real_images = real_images*2 - 1\n",
        "    \n",
        "    \n",
        "    # TRAINING DISCRIMINATOR\n",
        "    d_optimizer.zero_grad()\n",
        "    \n",
        "    \n",
        "    # 1. Train with Real Images\n",
        "    \n",
        "    # Compute the discriminator loss on real images\n",
        "    # Smooth the real labels\n",
        "    D_real = D(real_images)\n",
        "    d_real_loss = real_loss(D_real, smooth=True)\n",
        "    \n",
        "    \n",
        "    # 2. Train with fake images\n",
        "    \n",
        "    \n",
        "    # Generate fake images\n",
        "    z = np.random.uniform(-1, 1, size =(batch_size, z_size))\n",
        "    z = torch.from_numpy(z).float()\n",
        "    fake_images = G(z)\n",
        "    \n",
        "    # Compute the discriminator losses on fake images\n",
        "    D_fake = D(fake_images)\n",
        "    d_fake_loss = fake_loss(D_fake)\n",
        "    \n",
        "    # Adding up fake losses and perform backpropagation\n",
        "    d_loss = d_real+loss + d_fake_loss\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "    \n",
        "    \n",
        "    # TRAINING GENERATOR\n",
        "    g_optimizer.zero_grad()\n",
        "    \n",
        "    \n",
        "    # 1. Train with fake images and flipped labels\n",
        "    \n",
        "    \n",
        "    # Generate fake labels\n",
        "    z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "    z = torch.from_numpy(z).float()\n",
        "    fake_images = G(z)\n",
        "    \n",
        "    # Compute discriminator losses on fake images using flipped labels\n",
        "    D_fake = D(fake_images)\n",
        "    g_loss = real_loss(D_fake)    # Using real loss to flip labels\n",
        "    \n",
        "    # Perform backprop\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "    \n",
        "    # Printing some loss stats\n",
        "    if batch_i % print_every == 0:\n",
        "      # Print discriminator and generator loss\n",
        "      print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "      \n",
        "      \n",
        "    # After each Epoch\n",
        "    # Append discriminator and generator loss\n",
        "    losses.append((d_loss.item(), g_loss.item()))\n",
        "    \n",
        "    # Generate and save sample, fake images\n",
        "    G.eval()  # Evaluate mode for generating samples\n",
        "    samples_z = G(fixed_z)\n",
        "    samples.append(samples_z)\n",
        "    G.train()   # Back to train code\n",
        "    \n",
        "    \n",
        "    \n",
        "# Save training generator samples\n",
        "with open('train_samples.pkl', 'wb') as f:\n",
        "  pkl.dump(samples, f)\n",
        "      \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-73a59f277624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Smooth the real labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0md_real_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-3d52de462eba>\u001b[0m in \u001b[0;36mreal_loss\u001b[0;34m(D_out, smooth)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mbatch_Size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;31m# label smoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# if smooth, real_labels = 0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 539\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Discriminator' object has no attribute 'out'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54K4m9m1PlI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}