{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan-mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6NX08GRfRli",
        "colab_type": "text"
      },
      "source": [
        "#GAN - MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzajUrQPpHKA",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0xXTO7ijGHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdLFetGwpJka",
        "colab_type": "text"
      },
      "source": [
        "##Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT0Pp1UljUPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Initialize Batch Size\n",
        "batch_size = 64\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Load Dataset\n",
        "train_dataset = datasets.MNIST(root='data', train=True, download =True, transform=transform)\n",
        "\n",
        "# Prepare the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_workers )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh0dtKrSpL81",
        "colab_type": "text"
      },
      "source": [
        "##Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouajGzj3lD-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "a96e7e9a-c7f1-41a9-85b6-07f3d524b839"
      },
      "source": [
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "\n",
        "# get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "fig = plt.figure(figsize = (3,3)) \n",
        "px = fig.add_subplot(111)\n",
        "px.imshow(img, cmap='gray')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f10f61b9240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADFCAYAAAARxr1AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JLQbBVsfIee",
        "colab_type": "text"
      },
      "source": [
        "#GAN Model\n",
        "##Discriminator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvZKA89_gFpJ",
        "colab_type": "text"
      },
      "source": [
        "It is a linear classifier with 2 hidden layers. This network uses leaky ReLU as an activation function.\n",
        "A leaky ReLU is a normal ReLU function but it return small non-zero value for negative inputs. We use leaky ReLU so gradients can backward propagate without hinderance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-NsvpxokELz",
        "colab_type": "text"
      },
      "source": [
        "Using a BCEWithLogitsLoss which combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable. Source: [Documentation](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIXXb3PCobrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(Discriminator, self).__init__()\n",
        "    \n",
        "    # Layers\n",
        "    self.conv1 = nn.Linear(input_size, hidden_dim*4)    # Input Layer\n",
        "    self.conv2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
        "    self.conv3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "    self.conv4 = nn.Linear(hidden_dim, output_size)  # Final Layer\n",
        "    \n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # Flatten image\n",
        "    x = x.view(-1, 28*28)\n",
        "      \n",
        "    x = F.leaky_relu(self.conv1(x), 0.2)   # (input, negative_slope)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "     \n",
        "    # Final Layer\n",
        "    out = self.conv4(x)\n",
        "    return out\n",
        "    \n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRe74OSRxRu3",
        "colab_type": "text"
      },
      "source": [
        "##Add Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atwr3f4-xVzb",
        "colab_type": "text"
      },
      "source": [
        "Works like the Discriminator but use tanh as an activation function in the output layer.\n",
        "Generator works best when tanh is scaled between -1 and 1.\n",
        "So we will have to scale our real input images to have pixel values between -1 and 1 when we train the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cf2_ZpmwBK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(Generator, self).__init__()\n",
        "    \n",
        "    # Layers\n",
        "    self.conv1 = nn.Linear(input_size, hidden_dim)    # Input Layer\n",
        "    self.conv2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
        "    self.conv3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n",
        "    self.conv4 = nn.Linear(hidden_dim*4, output_size)  # Final Layer\n",
        "    \n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.leaky_relu(self.conv1(x), 0.2)   # (input, negative_slope)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "     \n",
        "    # Final Layer\n",
        "    output = F.tanh(self.conv4(x))\n",
        "      \n",
        "    return output\n",
        "    \n",
        "   \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhKJivAX1Sz6",
        "colab_type": "text"
      },
      "source": [
        "##Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT5uVBitzZlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator Hyperarameters\n",
        "\n",
        "# Size of input image\n",
        "input_size = 784\n",
        "# Size of discriminator output\n",
        "d_output_size = 1\n",
        "# Size of last hidden layer in discriminator (layer 3)\n",
        "d_hidden_size = 32\n",
        "\n",
        "\n",
        "# Generator Hyperparamters\n",
        "\n",
        "# Size of latent vector to give to Generator (reffered as noise)\n",
        "z_size = 100\n",
        "# Size of discriminator output (generated image)\n",
        "g_output_size = 784\n",
        "# Size of first hidden layer\n",
        "g_hidden_size = 32\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tPGG7ZJ9m6e",
        "colab_type": "text"
      },
      "source": [
        "##Building Network\n",
        "Instantiating the current network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GvnM5aq9Smb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "221425a6-2440-45ec-ee9e-f3aa9d848594"
      },
      "source": [
        "D = Discriminator(input_size, d_hidden_size, d_output_size)\n",
        "G = Generator(z_size, g_hidden_size, g_output_size)\n",
        "\n",
        "print(D)\n",
        "print()\n",
        "print(G)\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (conv1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (conv2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (conv3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (conv4): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.3)\n",
            ")\n",
            "\n",
            "Generator(\n",
            "  (conv1): Linear(in_features=100, out_features=32, bias=True)\n",
            "  (conv2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (conv3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (conv4): Linear(in_features=128, out_features=784, bias=True)\n",
            "  (dropout): Dropout(p=0.3)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc7Dhd7rxBPl",
        "colab_type": "text"
      },
      "source": [
        "##Discriminator and Generator Losses\n",
        "###Discriminator Loss\n",
        "*  Total loss = real loss + fake loss (d_loss = d_real_loss + d_fake_loss)\n",
        "*   Output = 1 for real images\n",
        "*   Output = 0 for fake images\n",
        "*   Using BCEWithLogitsLoss\n",
        "*   To generalize better we reduce the real label classification limit to 0.9 - 1.0, using *smooth* parmeter for this\n",
        "*   Generator outputs images, we want D(fake_images) = 0\n",
        "\n",
        "###Generator Loss\n",
        "*   Similar to Discriminator only flipped, D(fake_images) = 1\n",
        "*   The labels are flipped to represent that the generator is trying to fool the discriminator into thinking that the images it generates (fakes) are real!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EltQcy1e-TFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Calculating losses\n",
        "def real_loss(D_out, smooth=False):\n",
        "    batch_size = D_out.size(0)\n",
        "    # label smoothing\n",
        "    if smooth:\n",
        "        # smooth, real labels = 0.9\n",
        "        labels = torch.ones(batch_size)*0.9\n",
        "    else:\n",
        "        labels = torch.ones(batch_size) # real labels = 1\n",
        "        \n",
        "    # numerically stable loss\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # calculate loss\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n",
        "\n",
        "def fake_loss(D_out):\n",
        "    batch_size = D_out.size(0)\n",
        "    labels = torch.zeros(batch_size) # fake labels = 0\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # calculate loss\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjUBhCqpl3Mc",
        "colab_type": "text"
      },
      "source": [
        "##Optimizers\n",
        "Updating generator and discriminator separately. Assigning two separate Adam optimizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqJbeEOBL55d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Learning Rate\n",
        "lr = 0.002\n",
        "\n",
        "# Optimizers\n",
        "d_optimizer = optim.Adam(D.parameters(), lr)\n",
        "g_optimizer = optim.Adam(G.parameters(), lr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy2h4rrQfudK",
        "colab_type": "text"
      },
      "source": [
        "##Training\n",
        "We will use real_loss and fake_loss to calculate losses in all the following cases\n",
        "###Discriminator\n",
        "*   Compute discriminator loss on real, training images\n",
        "*   Generate fake images\n",
        "*   Compute loss on fake (generated) images\n",
        "*   Add real and fake loss\n",
        "*   Perform backpropagation + an optimization step to update discriminator weights\n",
        "\n",
        "###Generator\n",
        "*   Generate fake images\n",
        "*   Compute discriminator loss on fake (generated) images\n",
        "*   Perform backpropagation + an optimization step to update generator weights\n",
        "\n",
        "###Saving Samples\n",
        "As we train, we printout loss statistics and save images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTX72hbaL4-c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5151
        },
        "outputId": "84068b46-83c7-4f35-9dbe-4d8bde780ee3"
      },
      "source": [
        "import pickle as pkl\n",
        "\n",
        "# Defining Epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# To keep track of loss and generated \"fake\" images\n",
        "samples = []\n",
        "losses = []\n",
        "\n",
        "print_every = 400\n",
        "\n",
        "# Getting fixed data fro sampling (constant unaltered throughtout training, allows to inspect the model's performance)\n",
        "\n",
        "sample_size = 16\n",
        "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
        "fixed_z = torch.from_numpy(fixed_z).float()\n",
        "\n",
        "# train the network\n",
        "D.train()\n",
        "G.train()\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  for batch_i, (real_images, _) in enumerate(train_loader):\n",
        "    \n",
        "    batch_size = real_images.size(0)\n",
        "    \n",
        "    \n",
        "    # Rescale Images from [0,1) to [-1, 1]\n",
        "    real_images = real_images*2 - 1\n",
        "    \n",
        "    \n",
        "    # TRAINING DISCRIMINATOR\n",
        "    d_optimizer.zero_grad()\n",
        "    \n",
        "    \n",
        "    # 1. Train with Real Images\n",
        "    \n",
        "    # Compute the discriminator loss on real images\n",
        "    # Smooth the real labels\n",
        "    D_real = D(real_images)\n",
        "    d_real_loss = real_loss(D_real, smooth=True)\n",
        "    \n",
        "    \n",
        "    # 2. Train with fake images\n",
        "    \n",
        "    \n",
        "    # Generate fake images\n",
        "    z = np.random.uniform(-1, 1, size =(batch_size, z_size))\n",
        "    z = torch.from_numpy(z).float()\n",
        "    fake_images = G(z)\n",
        "    \n",
        "    # Compute the discriminator losses on fake images\n",
        "    D_fake = D(fake_images)\n",
        "    d_fake_loss = fake_loss(D_fake)\n",
        "    \n",
        "    # Adding up fake losses and perform backpropagation\n",
        "    d_loss = d_real_loss + d_fake_loss\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "    \n",
        "    \n",
        "    # TRAINING GENERATOR\n",
        "    g_optimizer.zero_grad()\n",
        "    \n",
        "    \n",
        "    # 1. Train with fake images and flipped labels\n",
        "    \n",
        "    \n",
        "    # Generate fake labels\n",
        "    z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "    z = torch.from_numpy(z).float()\n",
        "    fake_images = G(z)\n",
        "    \n",
        "    # Compute discriminator losses on fake images using flipped labels\n",
        "    D_fake = D(fake_images)\n",
        "    g_loss = real_loss(D_fake)    # Using real loss to flip labels\n",
        "    \n",
        "    # Perform backprop\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "    \n",
        "    # Printing some loss stats\n",
        "    if batch_i % print_every == 0:\n",
        "      # Print discriminator and generator loss\n",
        "      print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "      \n",
        "      \n",
        "    # After each Epoch\n",
        "    # Append discriminator and generator loss\n",
        "    losses.append((d_loss.item(), g_loss.item()))\n",
        "    \n",
        "    # Generate and save sample, fake images\n",
        "    G.eval()  # Evaluate mode for generating samples\n",
        "    samples_z = G(fixed_z)\n",
        "    samples.append(samples_z)\n",
        "    G.train()   # Back to train code\n",
        "    \n",
        "    \n",
        "    \n",
        "# Save training generator samples\n",
        "with open('train_samples.pkl', 'wb') as f:\n",
        "  pkl.dump(samples, f)\n",
        "      \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [    1/  100] | d_loss: 1.2262 | g_loss: 0.9189\n",
            "Epoch [    1/  100] | d_loss: 1.1549 | g_loss: 1.2112\n",
            "Epoch [    1/  100] | d_loss: 1.2783 | g_loss: 1.2794\n",
            "Epoch [    2/  100] | d_loss: 1.3418 | g_loss: 1.2232\n",
            "Epoch [    2/  100] | d_loss: 1.2167 | g_loss: 1.1580\n",
            "Epoch [    2/  100] | d_loss: 1.2815 | g_loss: 0.9193\n",
            "Epoch [    3/  100] | d_loss: 1.3274 | g_loss: 0.8759\n",
            "Epoch [    3/  100] | d_loss: 1.1271 | g_loss: 1.2629\n",
            "Epoch [    3/  100] | d_loss: 1.4133 | g_loss: 1.2131\n",
            "Epoch [    4/  100] | d_loss: 1.3011 | g_loss: 0.9382\n",
            "Epoch [    4/  100] | d_loss: 1.2151 | g_loss: 0.8853\n",
            "Epoch [    4/  100] | d_loss: 1.4740 | g_loss: 1.1774\n",
            "Epoch [    5/  100] | d_loss: 1.2151 | g_loss: 1.1024\n",
            "Epoch [    5/  100] | d_loss: 1.3864 | g_loss: 0.9875\n",
            "Epoch [    5/  100] | d_loss: 1.1121 | g_loss: 1.2611\n",
            "Epoch [    6/  100] | d_loss: 1.3218 | g_loss: 1.9338\n",
            "Epoch [    6/  100] | d_loss: 1.3079 | g_loss: 1.0869\n",
            "Epoch [    6/  100] | d_loss: 1.3013 | g_loss: 1.0632\n",
            "Epoch [    7/  100] | d_loss: 1.2791 | g_loss: 0.9279\n",
            "Epoch [    7/  100] | d_loss: 1.2106 | g_loss: 1.0198\n",
            "Epoch [    7/  100] | d_loss: 1.3658 | g_loss: 0.9197\n",
            "Epoch [    8/  100] | d_loss: 1.3101 | g_loss: 0.8746\n",
            "Epoch [    8/  100] | d_loss: 1.2035 | g_loss: 1.2894\n",
            "Epoch [    8/  100] | d_loss: 1.3418 | g_loss: 0.9286\n",
            "Epoch [    9/  100] | d_loss: 1.1992 | g_loss: 1.1609\n",
            "Epoch [    9/  100] | d_loss: 1.2787 | g_loss: 1.0089\n",
            "Epoch [    9/  100] | d_loss: 1.3075 | g_loss: 1.0326\n",
            "Epoch [   10/  100] | d_loss: 1.2798 | g_loss: 1.1906\n",
            "Epoch [   10/  100] | d_loss: 1.1791 | g_loss: 1.0480\n",
            "Epoch [   10/  100] | d_loss: 1.2635 | g_loss: 1.2743\n",
            "Epoch [   11/  100] | d_loss: 1.2158 | g_loss: 0.9380\n",
            "Epoch [   11/  100] | d_loss: 1.3011 | g_loss: 0.9158\n",
            "Epoch [   11/  100] | d_loss: 1.4062 | g_loss: 1.2723\n",
            "Epoch [   12/  100] | d_loss: 1.2933 | g_loss: 0.9612\n",
            "Epoch [   12/  100] | d_loss: 1.2891 | g_loss: 1.2067\n",
            "Epoch [   12/  100] | d_loss: 1.2672 | g_loss: 1.1006\n",
            "Epoch [   13/  100] | d_loss: 1.2321 | g_loss: 1.0121\n",
            "Epoch [   13/  100] | d_loss: 1.3042 | g_loss: 1.1497\n",
            "Epoch [   13/  100] | d_loss: 1.4240 | g_loss: 1.0311\n",
            "Epoch [   14/  100] | d_loss: 1.2037 | g_loss: 1.2983\n",
            "Epoch [   14/  100] | d_loss: 1.3328 | g_loss: 1.0308\n",
            "Epoch [   14/  100] | d_loss: 1.2970 | g_loss: 1.4178\n",
            "Epoch [   15/  100] | d_loss: 1.2456 | g_loss: 1.1996\n",
            "Epoch [   15/  100] | d_loss: 1.1697 | g_loss: 0.9898\n",
            "Epoch [   15/  100] | d_loss: 1.3372 | g_loss: 0.8253\n",
            "Epoch [   16/  100] | d_loss: 1.2786 | g_loss: 1.0182\n",
            "Epoch [   16/  100] | d_loss: 1.2753 | g_loss: 0.9693\n",
            "Epoch [   16/  100] | d_loss: 1.4856 | g_loss: 1.0202\n",
            "Epoch [   17/  100] | d_loss: 1.2045 | g_loss: 1.2025\n",
            "Epoch [   17/  100] | d_loss: 1.1478 | g_loss: 1.2335\n",
            "Epoch [   17/  100] | d_loss: 1.3148 | g_loss: 1.0758\n",
            "Epoch [   18/  100] | d_loss: 1.4015 | g_loss: 1.9312\n",
            "Epoch [   18/  100] | d_loss: 1.2502 | g_loss: 0.9038\n",
            "Epoch [   18/  100] | d_loss: 1.3945 | g_loss: 0.9802\n",
            "Epoch [   19/  100] | d_loss: 1.1933 | g_loss: 0.9812\n",
            "Epoch [   19/  100] | d_loss: 1.2939 | g_loss: 0.9126\n",
            "Epoch [   19/  100] | d_loss: 1.2493 | g_loss: 1.2345\n",
            "Epoch [   20/  100] | d_loss: 1.1933 | g_loss: 1.0351\n",
            "Epoch [   20/  100] | d_loss: 1.2616 | g_loss: 1.3111\n",
            "Epoch [   20/  100] | d_loss: 1.2312 | g_loss: 1.4791\n",
            "Epoch [   21/  100] | d_loss: 1.2635 | g_loss: 1.0108\n",
            "Epoch [   21/  100] | d_loss: 1.3769 | g_loss: 1.1102\n",
            "Epoch [   21/  100] | d_loss: 1.4120 | g_loss: 1.2297\n",
            "Epoch [   22/  100] | d_loss: 1.2198 | g_loss: 1.2548\n",
            "Epoch [   22/  100] | d_loss: 1.2748 | g_loss: 1.2071\n",
            "Epoch [   22/  100] | d_loss: 1.3592 | g_loss: 1.0907\n",
            "Epoch [   23/  100] | d_loss: 1.3036 | g_loss: 1.0772\n",
            "Epoch [   23/  100] | d_loss: 1.2090 | g_loss: 1.1388\n",
            "Epoch [   23/  100] | d_loss: 1.4661 | g_loss: 1.1671\n",
            "Epoch [   24/  100] | d_loss: 1.2426 | g_loss: 0.9134\n",
            "Epoch [   24/  100] | d_loss: 1.2678 | g_loss: 1.0837\n",
            "Epoch [   24/  100] | d_loss: 1.2976 | g_loss: 1.0072\n",
            "Epoch [   25/  100] | d_loss: 1.2728 | g_loss: 0.8941\n",
            "Epoch [   25/  100] | d_loss: 1.2705 | g_loss: 0.9111\n",
            "Epoch [   25/  100] | d_loss: 1.3501 | g_loss: 0.9174\n",
            "Epoch [   26/  100] | d_loss: 1.2913 | g_loss: 0.8001\n",
            "Epoch [   26/  100] | d_loss: 1.2276 | g_loss: 1.0759\n",
            "Epoch [   26/  100] | d_loss: 1.2827 | g_loss: 0.8683\n",
            "Epoch [   27/  100] | d_loss: 1.1795 | g_loss: 0.9870\n",
            "Epoch [   27/  100] | d_loss: 1.3353 | g_loss: 1.3511\n",
            "Epoch [   27/  100] | d_loss: 1.3865 | g_loss: 1.1538\n",
            "Epoch [   28/  100] | d_loss: 1.3006 | g_loss: 1.0373\n",
            "Epoch [   28/  100] | d_loss: 1.1533 | g_loss: 1.0642\n",
            "Epoch [   28/  100] | d_loss: 1.3230 | g_loss: 0.9838\n",
            "Epoch [   29/  100] | d_loss: 1.3365 | g_loss: 1.2476\n",
            "Epoch [   29/  100] | d_loss: 1.1540 | g_loss: 0.9350\n",
            "Epoch [   29/  100] | d_loss: 1.3138 | g_loss: 1.0773\n",
            "Epoch [   30/  100] | d_loss: 1.2811 | g_loss: 0.9173\n",
            "Epoch [   30/  100] | d_loss: 1.3177 | g_loss: 1.0472\n",
            "Epoch [   30/  100] | d_loss: 1.3922 | g_loss: 1.0112\n",
            "Epoch [   31/  100] | d_loss: 1.2410 | g_loss: 1.2588\n",
            "Epoch [   31/  100] | d_loss: 1.1632 | g_loss: 1.0231\n",
            "Epoch [   31/  100] | d_loss: 1.2734 | g_loss: 0.8512\n",
            "Epoch [   32/  100] | d_loss: 1.4961 | g_loss: 1.6984\n",
            "Epoch [   32/  100] | d_loss: 1.2088 | g_loss: 1.0519\n",
            "Epoch [   32/  100] | d_loss: 1.2813 | g_loss: 1.1159\n",
            "Epoch [   33/  100] | d_loss: 1.2900 | g_loss: 1.0260\n",
            "Epoch [   33/  100] | d_loss: 1.3369 | g_loss: 0.9355\n",
            "Epoch [   33/  100] | d_loss: 1.3392 | g_loss: 0.9941\n",
            "Epoch [   34/  100] | d_loss: 1.2484 | g_loss: 1.2051\n",
            "Epoch [   34/  100] | d_loss: 1.2445 | g_loss: 1.1074\n",
            "Epoch [   34/  100] | d_loss: 1.3901 | g_loss: 1.1580\n",
            "Epoch [   35/  100] | d_loss: 1.2227 | g_loss: 1.3670\n",
            "Epoch [   35/  100] | d_loss: 1.1937 | g_loss: 1.1351\n",
            "Epoch [   35/  100] | d_loss: 1.2429 | g_loss: 1.1242\n",
            "Epoch [   36/  100] | d_loss: 1.3245 | g_loss: 1.4437\n",
            "Epoch [   36/  100] | d_loss: 1.2715 | g_loss: 1.1123\n",
            "Epoch [   36/  100] | d_loss: 1.3600 | g_loss: 1.2199\n",
            "Epoch [   37/  100] | d_loss: 1.2278 | g_loss: 1.3453\n",
            "Epoch [   37/  100] | d_loss: 1.3258 | g_loss: 0.9625\n",
            "Epoch [   37/  100] | d_loss: 1.2592 | g_loss: 1.2035\n",
            "Epoch [   38/  100] | d_loss: 1.1908 | g_loss: 1.1236\n",
            "Epoch [   38/  100] | d_loss: 1.2502 | g_loss: 0.9673\n",
            "Epoch [   38/  100] | d_loss: 1.2294 | g_loss: 1.1266\n",
            "Epoch [   39/  100] | d_loss: 1.2680 | g_loss: 0.8924\n",
            "Epoch [   39/  100] | d_loss: 1.0325 | g_loss: 2.5613\n",
            "Epoch [   39/  100] | d_loss: 1.3175 | g_loss: 1.0108\n",
            "Epoch [   40/  100] | d_loss: 1.3003 | g_loss: 0.8421\n",
            "Epoch [   40/  100] | d_loss: 1.1587 | g_loss: 1.2090\n",
            "Epoch [   40/  100] | d_loss: 1.3128 | g_loss: 0.9789\n",
            "Epoch [   41/  100] | d_loss: 1.3790 | g_loss: 0.8976\n",
            "Epoch [   41/  100] | d_loss: 1.0913 | g_loss: 0.9416\n",
            "Epoch [   41/  100] | d_loss: 1.3972 | g_loss: 0.9190\n",
            "Epoch [   42/  100] | d_loss: 1.2872 | g_loss: 1.2286\n",
            "Epoch [   42/  100] | d_loss: 1.3156 | g_loss: 1.0677\n",
            "Epoch [   42/  100] | d_loss: 1.3012 | g_loss: 0.9961\n",
            "Epoch [   43/  100] | d_loss: 1.2699 | g_loss: 1.0566\n",
            "Epoch [   43/  100] | d_loss: 1.3397 | g_loss: 1.0956\n",
            "Epoch [   43/  100] | d_loss: 1.2920 | g_loss: 1.2193\n",
            "Epoch [   44/  100] | d_loss: 1.2679 | g_loss: 1.0831\n",
            "Epoch [   44/  100] | d_loss: 1.1999 | g_loss: 1.1365\n",
            "Epoch [   44/  100] | d_loss: 1.2370 | g_loss: 1.2137\n",
            "Epoch [   45/  100] | d_loss: 1.2559 | g_loss: 0.8547\n",
            "Epoch [   45/  100] | d_loss: 1.2130 | g_loss: 0.9343\n",
            "Epoch [   45/  100] | d_loss: 1.2898 | g_loss: 1.1523\n",
            "Epoch [   46/  100] | d_loss: 1.5196 | g_loss: 1.0470\n",
            "Epoch [   46/  100] | d_loss: 1.2458 | g_loss: 1.3280\n",
            "Epoch [   46/  100] | d_loss: 1.3853 | g_loss: 1.1205\n",
            "Epoch [   47/  100] | d_loss: 1.1700 | g_loss: 1.1190\n",
            "Epoch [   47/  100] | d_loss: 1.1739 | g_loss: 1.0975\n",
            "Epoch [   47/  100] | d_loss: 1.1951 | g_loss: 1.0971\n",
            "Epoch [   48/  100] | d_loss: 1.2905 | g_loss: 1.3007\n",
            "Epoch [   48/  100] | d_loss: 1.2579 | g_loss: 1.0743\n",
            "Epoch [   48/  100] | d_loss: 1.2444 | g_loss: 1.1471\n",
            "Epoch [   49/  100] | d_loss: 1.2584 | g_loss: 1.3600\n",
            "Epoch [   49/  100] | d_loss: 1.2199 | g_loss: 1.0990\n",
            "Epoch [   49/  100] | d_loss: 1.3745 | g_loss: 0.8873\n",
            "Epoch [   50/  100] | d_loss: 1.3824 | g_loss: 1.6741\n",
            "Epoch [   50/  100] | d_loss: 1.1999 | g_loss: 1.0262\n",
            "Epoch [   50/  100] | d_loss: 1.3182 | g_loss: 1.3265\n",
            "Epoch [   51/  100] | d_loss: 1.1852 | g_loss: 1.0863\n",
            "Epoch [   51/  100] | d_loss: 1.2302 | g_loss: 0.9777\n",
            "Epoch [   51/  100] | d_loss: 1.3471 | g_loss: 0.8766\n",
            "Epoch [   52/  100] | d_loss: 1.1428 | g_loss: 1.0192\n",
            "Epoch [   52/  100] | d_loss: 1.1820 | g_loss: 0.9789\n",
            "Epoch [   52/  100] | d_loss: 1.1555 | g_loss: 1.2069\n",
            "Epoch [   53/  100] | d_loss: 1.1961 | g_loss: 1.1183\n",
            "Epoch [   53/  100] | d_loss: 1.1728 | g_loss: 0.9610\n",
            "Epoch [   53/  100] | d_loss: 1.3184 | g_loss: 1.1819\n",
            "Epoch [   54/  100] | d_loss: 1.2025 | g_loss: 1.2738\n",
            "Epoch [   54/  100] | d_loss: 1.2231 | g_loss: 0.9616\n",
            "Epoch [   54/  100] | d_loss: 1.3694 | g_loss: 1.0440\n",
            "Epoch [   55/  100] | d_loss: 1.3225 | g_loss: 1.3239\n",
            "Epoch [   55/  100] | d_loss: 1.4088 | g_loss: 1.1020\n",
            "Epoch [   55/  100] | d_loss: 1.3114 | g_loss: 1.0114\n",
            "Epoch [   56/  100] | d_loss: 1.2207 | g_loss: 1.0332\n",
            "Epoch [   56/  100] | d_loss: 1.2485 | g_loss: 1.1271\n",
            "Epoch [   56/  100] | d_loss: 1.3694 | g_loss: 1.0436\n",
            "Epoch [   57/  100] | d_loss: 1.2186 | g_loss: 1.3843\n",
            "Epoch [   57/  100] | d_loss: 1.2996 | g_loss: 1.0292\n",
            "Epoch [   57/  100] | d_loss: 1.3650 | g_loss: 0.9823\n",
            "Epoch [   58/  100] | d_loss: 1.2361 | g_loss: 1.4350\n",
            "Epoch [   58/  100] | d_loss: 1.1958 | g_loss: 1.0107\n",
            "Epoch [   58/  100] | d_loss: 1.3242 | g_loss: 1.0391\n",
            "Epoch [   59/  100] | d_loss: 1.2502 | g_loss: 1.0204\n",
            "Epoch [   59/  100] | d_loss: 1.2953 | g_loss: 1.3409\n",
            "Epoch [   59/  100] | d_loss: 1.3670 | g_loss: 1.1266\n",
            "Epoch [   60/  100] | d_loss: 1.2359 | g_loss: 0.8289\n",
            "Epoch [   60/  100] | d_loss: 1.1675 | g_loss: 1.0202\n",
            "Epoch [   60/  100] | d_loss: 1.3652 | g_loss: 0.9817\n",
            "Epoch [   61/  100] | d_loss: 1.2560 | g_loss: 1.2731\n",
            "Epoch [   61/  100] | d_loss: 1.2297 | g_loss: 1.2174\n",
            "Epoch [   61/  100] | d_loss: 1.3606 | g_loss: 0.8819\n",
            "Epoch [   62/  100] | d_loss: 1.2786 | g_loss: 1.1759\n",
            "Epoch [   62/  100] | d_loss: 1.2232 | g_loss: 1.0743\n",
            "Epoch [   62/  100] | d_loss: 1.3638 | g_loss: 1.0648\n",
            "Epoch [   63/  100] | d_loss: 1.2660 | g_loss: 0.9443\n",
            "Epoch [   63/  100] | d_loss: 1.1949 | g_loss: 1.1678\n",
            "Epoch [   63/  100] | d_loss: 1.2722 | g_loss: 1.1976\n",
            "Epoch [   64/  100] | d_loss: 1.3241 | g_loss: 1.2592\n",
            "Epoch [   64/  100] | d_loss: 1.1467 | g_loss: 1.1353\n",
            "Epoch [   64/  100] | d_loss: 1.2912 | g_loss: 0.9740\n",
            "Epoch [   65/  100] | d_loss: 1.3361 | g_loss: 1.0843\n",
            "Epoch [   65/  100] | d_loss: 1.2463 | g_loss: 0.9196\n",
            "Epoch [   65/  100] | d_loss: 1.2586 | g_loss: 0.9025\n",
            "Epoch [   66/  100] | d_loss: 1.2240 | g_loss: 1.5828\n",
            "Epoch [   66/  100] | d_loss: 1.1511 | g_loss: 1.1378\n",
            "Epoch [   66/  100] | d_loss: 1.2340 | g_loss: 1.4108\n",
            "Epoch [   67/  100] | d_loss: 1.2489 | g_loss: 0.9247\n",
            "Epoch [   67/  100] | d_loss: 1.2309 | g_loss: 0.9385\n",
            "Epoch [   67/  100] | d_loss: 1.2951 | g_loss: 1.1716\n",
            "Epoch [   68/  100] | d_loss: 1.2831 | g_loss: 0.9746\n",
            "Epoch [   68/  100] | d_loss: 1.1921 | g_loss: 1.1423\n",
            "Epoch [   68/  100] | d_loss: 1.2996 | g_loss: 1.0141\n",
            "Epoch [   69/  100] | d_loss: 1.2193 | g_loss: 1.5023\n",
            "Epoch [   69/  100] | d_loss: 1.2284 | g_loss: 0.8859\n",
            "Epoch [   69/  100] | d_loss: 1.2847 | g_loss: 1.1014\n",
            "Epoch [   70/  100] | d_loss: 1.3072 | g_loss: 1.1503\n",
            "Epoch [   70/  100] | d_loss: 1.0260 | g_loss: 0.9940\n",
            "Epoch [   70/  100] | d_loss: 1.2754 | g_loss: 0.9562\n",
            "Epoch [   71/  100] | d_loss: 1.2666 | g_loss: 1.2003\n",
            "Epoch [   71/  100] | d_loss: 1.1475 | g_loss: 0.9483\n",
            "Epoch [   71/  100] | d_loss: 1.4166 | g_loss: 1.2186\n",
            "Epoch [   72/  100] | d_loss: 1.1727 | g_loss: 0.9469\n",
            "Epoch [   72/  100] | d_loss: 1.2458 | g_loss: 1.1145\n",
            "Epoch [   72/  100] | d_loss: 1.2958 | g_loss: 1.1021\n",
            "Epoch [   73/  100] | d_loss: 1.2645 | g_loss: 1.0909\n",
            "Epoch [   73/  100] | d_loss: 1.1870 | g_loss: 0.9479\n",
            "Epoch [   73/  100] | d_loss: 1.2692 | g_loss: 0.9321\n",
            "Epoch [   74/  100] | d_loss: 1.2303 | g_loss: 1.2376\n",
            "Epoch [   74/  100] | d_loss: 1.2409 | g_loss: 1.2353\n",
            "Epoch [   74/  100] | d_loss: 1.2582 | g_loss: 1.1720\n",
            "Epoch [   75/  100] | d_loss: 1.2227 | g_loss: 1.3073\n",
            "Epoch [   75/  100] | d_loss: 1.2294 | g_loss: 0.9905\n",
            "Epoch [   75/  100] | d_loss: 1.3976 | g_loss: 1.0554\n",
            "Epoch [   76/  100] | d_loss: 1.3376 | g_loss: 0.8163\n",
            "Epoch [   76/  100] | d_loss: 1.1979 | g_loss: 1.0027\n",
            "Epoch [   76/  100] | d_loss: 1.3649 | g_loss: 1.3907\n",
            "Epoch [   77/  100] | d_loss: 1.2278 | g_loss: 1.4479\n",
            "Epoch [   77/  100] | d_loss: 1.2710 | g_loss: 1.2268\n",
            "Epoch [   77/  100] | d_loss: 1.3094 | g_loss: 0.9904\n",
            "Epoch [   78/  100] | d_loss: 1.3057 | g_loss: 1.0306\n",
            "Epoch [   78/  100] | d_loss: 1.2823 | g_loss: 1.2080\n",
            "Epoch [   78/  100] | d_loss: 1.3728 | g_loss: 1.5224\n",
            "Epoch [   79/  100] | d_loss: 1.2670 | g_loss: 1.5153\n",
            "Epoch [   79/  100] | d_loss: 1.2577 | g_loss: 1.0277\n",
            "Epoch [   79/  100] | d_loss: 1.4330 | g_loss: 1.0790\n",
            "Epoch [   80/  100] | d_loss: 1.3022 | g_loss: 1.0514\n",
            "Epoch [   80/  100] | d_loss: 1.2591 | g_loss: 1.3811\n",
            "Epoch [   80/  100] | d_loss: 1.3152 | g_loss: 0.9783\n",
            "Epoch [   81/  100] | d_loss: 1.2819 | g_loss: 0.8977\n",
            "Epoch [   81/  100] | d_loss: 1.2418 | g_loss: 1.0081\n",
            "Epoch [   81/  100] | d_loss: 1.3146 | g_loss: 0.9454\n",
            "Epoch [   82/  100] | d_loss: 1.2793 | g_loss: 1.1464\n",
            "Epoch [   82/  100] | d_loss: 1.2570 | g_loss: 0.9129\n",
            "Epoch [   82/  100] | d_loss: 1.2518 | g_loss: 1.2583\n",
            "Epoch [   83/  100] | d_loss: 1.2458 | g_loss: 1.3169\n",
            "Epoch [   83/  100] | d_loss: 1.1772 | g_loss: 1.1426\n",
            "Epoch [   83/  100] | d_loss: 1.3336 | g_loss: 1.0406\n",
            "Epoch [   84/  100] | d_loss: 1.2404 | g_loss: 0.9022\n",
            "Epoch [   84/  100] | d_loss: 1.1978 | g_loss: 1.0794\n",
            "Epoch [   84/  100] | d_loss: 1.2798 | g_loss: 1.1946\n",
            "Epoch [   85/  100] | d_loss: 1.2508 | g_loss: 1.0114\n",
            "Epoch [   85/  100] | d_loss: 1.2847 | g_loss: 0.9999\n",
            "Epoch [   85/  100] | d_loss: 1.3994 | g_loss: 0.9530\n",
            "Epoch [   86/  100] | d_loss: 1.2534 | g_loss: 0.9840\n",
            "Epoch [   86/  100] | d_loss: 1.2669 | g_loss: 0.9832\n",
            "Epoch [   86/  100] | d_loss: 1.2863 | g_loss: 1.0170\n",
            "Epoch [   87/  100] | d_loss: 1.2367 | g_loss: 1.1617\n",
            "Epoch [   87/  100] | d_loss: 1.2164 | g_loss: 1.4200\n",
            "Epoch [   87/  100] | d_loss: 1.3195 | g_loss: 0.9243\n",
            "Epoch [   88/  100] | d_loss: 1.3420 | g_loss: 1.4846\n",
            "Epoch [   88/  100] | d_loss: 1.2084 | g_loss: 1.0791\n",
            "Epoch [   88/  100] | d_loss: 1.3541 | g_loss: 0.8931\n",
            "Epoch [   89/  100] | d_loss: 1.2920 | g_loss: 1.0973\n",
            "Epoch [   89/  100] | d_loss: 1.2078 | g_loss: 1.2402\n",
            "Epoch [   89/  100] | d_loss: 1.2210 | g_loss: 0.9733\n",
            "Epoch [   90/  100] | d_loss: 1.2749 | g_loss: 0.7769\n",
            "Epoch [   90/  100] | d_loss: 1.1936 | g_loss: 0.9538\n",
            "Epoch [   90/  100] | d_loss: 1.3575 | g_loss: 0.9951\n",
            "Epoch [   91/  100] | d_loss: 1.4308 | g_loss: 1.4900\n",
            "Epoch [   91/  100] | d_loss: 1.1871 | g_loss: 1.0686\n",
            "Epoch [   91/  100] | d_loss: 1.2815 | g_loss: 0.9625\n",
            "Epoch [   92/  100] | d_loss: 1.2871 | g_loss: 1.1375\n",
            "Epoch [   92/  100] | d_loss: 1.2798 | g_loss: 1.1884\n",
            "Epoch [   92/  100] | d_loss: 1.3862 | g_loss: 0.9463\n",
            "Epoch [   93/  100] | d_loss: 1.2213 | g_loss: 1.3647\n",
            "Epoch [   93/  100] | d_loss: 1.2563 | g_loss: 1.0059\n",
            "Epoch [   93/  100] | d_loss: 1.3682 | g_loss: 1.2349\n",
            "Epoch [   94/  100] | d_loss: 1.1548 | g_loss: 1.1436\n",
            "Epoch [   94/  100] | d_loss: 1.2586 | g_loss: 1.0029\n",
            "Epoch [   94/  100] | d_loss: 1.4366 | g_loss: 1.0641\n",
            "Epoch [   95/  100] | d_loss: 1.2203 | g_loss: 1.0578\n",
            "Epoch [   95/  100] | d_loss: 1.2485 | g_loss: 0.9923\n",
            "Epoch [   95/  100] | d_loss: 1.3023 | g_loss: 0.9559\n",
            "Epoch [   96/  100] | d_loss: 1.2620 | g_loss: 1.2759\n",
            "Epoch [   96/  100] | d_loss: 1.0916 | g_loss: 1.0261\n",
            "Epoch [   96/  100] | d_loss: 1.3603 | g_loss: 1.3070\n",
            "Epoch [   97/  100] | d_loss: 1.2004 | g_loss: 1.0552\n",
            "Epoch [   97/  100] | d_loss: 1.2529 | g_loss: 1.1284\n",
            "Epoch [   97/  100] | d_loss: 1.4696 | g_loss: 1.1025\n",
            "Epoch [   98/  100] | d_loss: 1.2557 | g_loss: 1.3544\n",
            "Epoch [   98/  100] | d_loss: 1.2727 | g_loss: 0.9592\n",
            "Epoch [   98/  100] | d_loss: 1.2639 | g_loss: 1.1840\n",
            "Epoch [   99/  100] | d_loss: 1.3194 | g_loss: 1.1501\n",
            "Epoch [   99/  100] | d_loss: 1.1410 | g_loss: 1.0405\n",
            "Epoch [   99/  100] | d_loss: 1.2915 | g_loss: 1.0676\n",
            "Epoch [  100/  100] | d_loss: 1.2773 | g_loss: 1.0614\n",
            "Epoch [  100/  100] | d_loss: 1.2361 | g_loss: 0.9834\n",
            "Epoch [  100/  100] | d_loss: 1.3347 | g_loss: 1.2376\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}